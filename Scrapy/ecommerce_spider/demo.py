# demo.py
from scrapy.crawler import CrawlerProcess
from ecommerce_spider.spiders.shopify_crawl import ShopifyCrawlFastSpider


def run_batch(sites: list[dict]):
    """
    sites = [
        {"domain": "...", "category": "..."},
        {"domain": "...", "category": "..."},
    ]
    """

    process = CrawlerProcess(settings={
        # ==== æ€§èƒ½ï¼ˆæé€Ÿç‰ˆæ¨èï¼‰====
        "CONCURRENT_REQUESTS": 256,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 32,
        "DOWNLOAD_DELAY": 0,
        "AUTOTHROTTLE_ENABLED": False,
        "RETRY_TIMES": 3,
        "LOG_LEVEL": "INFO",
        "ROBOTSTXT_OBEY": False,
        "FEEDS": {},

        # ==== å¯¼å‡º ====
        "ITEM_PIPELINES": {
            "ecommerce_spider.pipelines.PandasExporter": 300,
        },
        "PANDAS_FIELDS": [
            "SKU", "Name", "Categories", "Regular price", "cf_opingts",
            "Description", "Images", "è‡ªå®šä¹‰åˆ†ç±»", "åŸç«™åŸŸå", "åˆ†å¸ƒç½‘ç«™è¯†åˆ«", "è¯­è¨€"
        ],
    })

    for site in sites:
        domain = site["domain"]
        category = site.get("category", "æœªçŸ¥åˆ†ç±»")

        site_name = domain.split("//")[-1].replace(".", "_")
        export_file = f"{site_name}.xlsx"

        process.crawl(
            ShopifyCrawlFastSpider,
            domain=domain,
            category=category,
            export_file=export_file,  # ğŸ‘ˆ å…³é”®
        )

    process.start()
    print("\nå…¨éƒ¨ç«™ç‚¹çˆ¬å–å®Œæˆ\n")


if __name__ == "__main__":
    sites = [
        # {"domain": "https://shibuya-stationery.com", "category": "åŠå…¬ç”¨å“"},
        # {"domain": "https://ewartwoods.com", "category": "åŠå…¬ç”¨å“"},
        {"domain": "https://www.lagirlusa.com", "category": "è‰ºæœ¯ä¸å¨±ä¹"},
        # {"domain": "https://www.bando.com", "category": "åŠå…¬ç”¨å“"},
        # {"domain": "https://tasklinesupplies.com", "category": "åŠå…¬ç”¨å“"},
    ]

    run_batch(sites)
